{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffda20ac-0759-4b83-b921-096f9693b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainIDs = (pd.read_csv('shaply/trainidx_adrenal.csv')).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5134f85-6c3a-4754-806b-6c425605b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset=pd.read_csv('Datasets/adrenalTumorData_red.csv',header=None)\n",
    "#dataset=pd.read_csv('shapley/ColoradoData_reduced.csv',header=None)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "weight_set=False\n",
    "#mtrx=pd.read_csv('shaply/omegaColorado_fold5.txt',header=None)\n",
    "data= (dataset.iloc[:,:-1])#.to_numpy()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "\n",
    "labels=(dataset.iloc[:,-1]).to_numpy()\n",
    "\n",
    "feature_names=[names for names in dataset.columns.to_numpy()[:-1]]\n",
    "data = data.apply(pd.to_numeric, errors='coerce',downcast='float')\n",
    "\n",
    "\n",
    "input_data = torch.tensor(data.to_numpy())  # Batch size of 1 for simplicity\n",
    "labels = torch.tensor(labels)\n",
    "num_classes = np.unique(labels.numpy()).size\n",
    "#print(input_data[0][0].dtype,model.fc.weight.dtype)\n",
    "\n",
    "# Example dimensions\n",
    "#input_dim = mtrx.shape[0]\n",
    "input_dim = len(feature_names)\n",
    "output_dim = num_classes\n",
    "\n",
    "# Assuming these are your predefined weights (you need to replace these with your actual weights)\n",
    "#init_weights_fc = torch.tensor(mtrx.to_numpy())\n",
    "one_hot = torch.nn.functional.one_hot(labels).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f750da5b-3b27-441d-98ea-ce5d7cad9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(input_data, labels, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661e428-876f-49fb-9501-0880f887d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weight():\n",
    "    #Set your custom initial weights\n",
    "    with torch.no_grad():\n",
    "        model.fc.weight.copy_(init_weights_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758a170c-bcd8-4275-b1d9-4c7caad5522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleLayerSoftmax(\n",
      "  (fc): Linear(in_features=11, out_features=61, bias=False)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc1): Linear(in_features=61, out_features=3, bias=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the structure of your single-layer model with softmax output\n",
    "class SingleLayerSoftmax(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SingleLayerSoftmax, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, input_dim+50, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(input_dim+50, output_dim,bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Apply softmax along the last dimension (dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        #out = self.sigmoid(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = SingleLayerSoftmax(input_dim, output_dim)\n",
    "print(model)\n",
    "\n",
    "# Example usage: forward pass with a random input\n",
    "#model=model.double()\n",
    "\n",
    "\n",
    "#for param in model.parameters():\n",
    "#    param.data = param.data.to(torch.int64)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Suitab for multi-class classification problems\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate of 0.001\n",
    "\n",
    "if weight_set:\n",
    "    set_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54430533-2cba-4f62-a811-5ddc7d68582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n,train_data,train_labels):\n",
    "# Training loop\n",
    "    for epoch in range(n):  # Train for 10 epochs\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(train_data)  # Forward pass\n",
    "        #print(outputs,train_labels)\n",
    "   \n",
    "        loss = criterion(outputs, train_labels)  # Calculate loss\n",
    "            \n",
    "        loss.backward()  # Backward pass to compute gradients        \n",
    "        optimizer.step()  # Update model parameters using Adam\n",
    "            \n",
    "        #print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "    #print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f1db95-40f2-455a-9022-112559cc9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "def test_model(test_data,test_labels):\n",
    "    # Forward pass to get predictions (raw scores)\n",
    "    predictions_raw = model(test_data)\n",
    "    # Convert raw scores into class indices (argmax along dimension 1, which represents classes)\n",
    "    _, predicted_classes = torch.max(predictions_raw, dim=1)\n",
    "    \n",
    "    accuracy_metric = torchmetrics.Accuracy(task='multiclass',num_classes=num_classes)\n",
    "    accuracy = accuracy_metric(predicted_classes, test_labels)\n",
    "    #print('test accuracy:')\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da738162-5b9f-4427-8580-0516ce846926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\subhashree\\AppData\\Local\\Temp\\ipykernel_13404\\755861978.py:27: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  np.savetxt('output\\WeightMatrix_AdrenalFold'+str(fold)+'.txt',weight_layer1, fmt= '%s',delimiter=',')\n",
      "C:\\Users\\subhashree\\AppData\\Local\\Temp\\ipykernel_13404\\755861978.py:29: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  np.savetxt('output\\ShapeProfileFold'+str(fold)+'.txt',x1_, fmt= '%s',delimiter=',')\n",
      "C:\\Users\\subhashree\\AppData\\Local\\Temp\\ipykernel_13404\\755861978.py:30: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  pd.DataFrame(trainID_fold).to_csv('output\\AdrenalFold5_trainIDs-wt',index=False, header=False)\n",
      "C:\\Users\\subhashree\\AppData\\Local\\Temp\\ipykernel_13404\\755861978.py:31: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  pd.DataFrame(trainID_fold).to_csv('output\\AdrenalFold5_testIDs-wt',index=False, header=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "    \n",
    "def train_inFold(fold, epoch):\n",
    "    if fold>1:\n",
    "        kf = KFold(n_splits=fold, shuffle=True, random_state=1)\n",
    "        trainID_fold=[]\n",
    "        testID_fold=[]\n",
    "        fold_datasets={}\n",
    "        fold_labels={}\n",
    "        #trainIDs = (pd.read_csv('shaply/trainidx_adrenal.csv',header=None)).to_numpy()\n",
    "        #for train_IDs in trainIDs:\n",
    "        #    cv_result = (model,data[train_IDs-2],labels[train_IDs-2],cv=3)\n",
    "        for fold, (trainID, testID) in enumerate(kf.split(input_data)):\n",
    "            #traintest_indices[fold]=[trainID,testID]\n",
    "            trainID_fold.append(trainID)\n",
    "            testID_fold.append(trainID)\n",
    "            #print([input_data[i] for i in trainID])\n",
    "            fold_datasets[fold]=[[input_data[i] for i in trainID],[input_data[i] for i in testID]]\n",
    "            fold_labels[fold]=[[labels[i] for i in trainID],[labels[i] for i in testID]]\n",
    "            #print(fold_labels)\n",
    "        for fold in range(len(fold_datasets.keys())):\n",
    "            #print(model(torch.stack(fold_datasets[fold][0])))\n",
    "            #print(weight_layer1)\n",
    "            train_model(epoch,torch.stack(fold_datasets[fold][0]),torch.stack(fold_labels[fold][0]))\n",
    "            test_model(torch.stack(fold_datasets[fold][0]),torch.stack(fold_labels[fold][0]))\n",
    "            weight_layer1 = model.fc.weight.detach().numpy()\n",
    "            np.savetxt('output\\WeightMatrix_AdrenalFold'+str(fold)+'.txt',weight_layer1, fmt= '%s',delimiter=',')\n",
    "            x1_=shap_profile()\n",
    "            np.savetxt('output\\ShapeProfileFold'+str(fold)+'.txt',x1_, fmt= '%s',delimiter=',')\n",
    "        pd.DataFrame(trainID_fold).to_csv('output\\AdrenalFold5_trainIDs-wt',index=False, header=False)\n",
    "        pd.DataFrame(trainID_fold).to_csv('output\\AdrenalFold5_testIDs-wt',index=False, header=False)\n",
    "        \n",
    "\n",
    "    elif fold==1:\n",
    "        #print(weight_layer1,mtrx,weight_layer2)\n",
    "        train_model(epoch,X_train,y_train)\n",
    "        test_model(X_valid,y_valid)\n",
    "\n",
    "    #df_x = pd.DataFrame(trainID_fold)\n",
    "    \n",
    "    #np.savetxt('trainIDs_AdrenalFold2',trainID_fold, fmt= '%s',delimiter=',')\n",
    "    #np.savetxt('testIDs_AdrenalFold2',testID_fold, fmt= '%s',delimiter=',')\n",
    "\n",
    "#weight_layer1 = model.fc.weight.detach().numpy()\n",
    "#weight_layer2 = model.fc1.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b903ffb6-9108-4f00-bef1-b34ae03858c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9255)\n",
      "tensor(0.8936)\n",
      "tensor(0.9149)\n",
      "tensor(0.9043)\n",
      "tensor(0.9202)\n"
     ]
    }
   ],
   "source": [
    "train_inFold(5,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602ea73b-3f9a-4dd3-87cf-8e8e134f189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "def featureSet_Permtns(feature_i):\n",
    "    feature_set = []\n",
    "    feature_name = feature_names.copy()\n",
    "    del feature_name[feature_i]\n",
    "    for i in range(len(feature_name)+1):\n",
    "        feature_set += it.combinations(feature_name,i)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a4840ac-158c-4592-9383-c474ed42e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSet_PermtnsValues(feature_set,n):\n",
    "    instance_set = []\n",
    "    label_set = []\n",
    "\n",
    "    for i in (range(len(feature_set))):\n",
    "        data1 = input_data[n].detach().numpy().copy()\n",
    "        indx = []\n",
    "        for j in range(len(feature_set[i])):\n",
    "            #print(j)\n",
    "            indx.append(feature_names.index(feature_set[i][j]))\n",
    "        for k in range(len(feature_names)):\n",
    "            if k not in indx:\n",
    "                data1[k] = 0\n",
    "                #print(k, data1)\n",
    "        #print(i,data1)\n",
    "        instance_set.append( data1)\n",
    "        label_set.append(labels[n])\n",
    "    return instance_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef8e758e-7a83-4eb4-afb5-6eeef40ed483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_value(feature_i,n):\n",
    "    #pred_data=model(input_data[n])#.detach().numpy()\n",
    "    featureSet = featureSet_Permtns(feature_i)\n",
    "\n",
    "    featureSet_WithFeature = featureSet_PermtnsValues(featureSet,n)\n",
    "    #permtns_pred=[]\n",
    "    #permtns_pred_withFeature=[]\n",
    "    sum_wof=0\n",
    "    \n",
    "     \n",
    "    for i in range(len(featureSet)):\n",
    "        featureSet_WithFeature[i][feature_i] = input_data[n][feature_i]\n",
    "    \n",
    "    #print('1',(featureSet_PermtnsValues(featureSet,n)),'2',featureSet_WithFeature)\n",
    "    permtns_pred = model(torch.tensor(featureSet_PermtnsValues(featureSet,n))).detach()\n",
    "    permtns_pred_withFeature = model(torch.tensor(featureSet_WithFeature)).detach()\n",
    "    \n",
    "\n",
    "    sum_wof = (criterion(permtns_pred, permtns_pred_withFeature))\n",
    "        #print('1',len(permtns_pred), '2',len(permtns_pred_withFeature))\n",
    "    #sum_wof=0\n",
    "    #sum_wf=0\n",
    "    #for i in range(len(permtns_pred)):\n",
    "     #   sum_wof+=((criterion(permtns_pred[i], permtns_pred_withFeature[i])))\n",
    "    #sum_=sum_wof/len(permtns_pred)\n",
    "    #for i in range(len(permtns_pred_withFeature)):\n",
    "        #sum_wf+=((criterion(permtns_pred_withFeature[n], permtns_pred_withFeature)))\n",
    "    #sum_wf/=len(permtns_pred_withFeature)\n",
    "    #sum_ = abs(abs(sum_wof)-abs(sum_wf))\n",
    "    #permtns_pred = clf.cross_val_predict(featureSet_PermtnsValues(featureSet,n))\n",
    "    #permtns_pred_withFeature = clf.cross_val_predict(featureSet_WithFeature)\n",
    "    #count = 0\n",
    "    sum_ = sum_wof/len(featureSet)\n",
    "    \n",
    "    #for i in range (len(permtns_pred)):\n",
    "    #    if any(permtns_pred_withFeature[i], permtns_pred[i]):\n",
    "    #        count+=1\n",
    "    #sum_= abs(abs(permtns_pred_withFeature)-abs(permtns_pred))\n",
    "    #if count !=0:\n",
    "    #    sip_feature_i = sum_/count\n",
    "    #else:\n",
    "    #    sip_feature_i = sum_\n",
    "    #return sip_feature_i\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cdd6c13-b4f5-4c39-bebd-58328966a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_profile():\n",
    "    shap=[0 for i in range(len(feature_names))]\n",
    "\n",
    "    for i in range (len(feature_names)):\n",
    "        for j in range (len(input_data)):\n",
    "            shap[i]+=abs(shap_value(i,j))\n",
    "        shap[i]=shap[i]/len(input_data)\n",
    "    return(shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0f44e2-86f4-4ce3-8e9e-c528c44fcf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subhashree\\AppData\\Local\\Temp\\ipykernel_13404\\1233899208.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  permtns_pred = model(torch.tensor(featureSet_PermtnsValues(featureSet,n))).detach()\n"
     ]
    }
   ],
   "source": [
    "x1_=shap_profile()\n",
    "#x_=[round((torch.norm(x1_[i])).detach().numpy()/(sum([torch.norm(x1_[i]) for i in range(len(x1_))])).detach().numpy(),3) for i in range(len(x1_))]\n",
    "#y_ =[round(i/sum(np.sum(abs(weight_layer1.dot(weight_layer1.T)),axis=1)),3) for i in np.sum(abs(weight_layer1.dot(weight_layer1.T)),axis=1)]\n",
    "#z_=np.array((abs(weight_layer1.dot(weight_layer1.T)).diagonal()))\n",
    "#x_ = [round(x1_[i].detach().numpy()/sum(np.sum(x1_[i].detach().numpy())),3) for i in range(len(x1_))]\n",
    "#y_ =[round(i/sum(np.sum(abs(weight_layer1.dot(weight_layer1.T)),axis=1)),3) for i in np.sum(abs(weight_layer1.dot(weight_layer1.T)),axis=1)]\n",
    "#y_ =[round(i/sum(np.sum(abs(weight_layer1.dot(weight_layer2.T)),axis=1)),3) for i in np.sum(abs(weight_layer1.dot(weight_layer2.T)),axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd231cb3-abc9-430a-ad0b-d6f8d3f3d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(x1_)),height=x1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287784b-f1aa-48df-aed9-20e31784296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_=[round((torch.norm(x1_[i])).detach().numpy()/(sum([torch.norm(x1_[i]) for i in range(len(x1_))])).detach().numpy(),3) for i in range(len(x1_))]\n",
    "#y_ =[round(i/sum(np.sum(abs(weight_layer1.dot(weight_layer1.T)),axis=1)),3) for i in np.sum(abs(weight_layer1.dot(weight_layer1.T)),axis=1)]\n",
    "z_=(np.array((abs(weight_layer1.dot(weight_layer1.T))).diagonal()))/sum(np.array((abs(weight_layer1.dot(weight_layer1.T))).diagonal()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c18d8d-7285-448f-8d9f-e6ce83e60d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe3b74-eaff-4993-b3d7-3b62e116dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sort = x_.copy()\n",
    "x_sort.sort(reverse=True)\n",
    "x_rank = [x_sort.index(i)+1 for i in x_]\n",
    "y_sort = y_.copy()\n",
    "y_sort.sort(reverse=True)\n",
    "y_rank = [y_sort.index(i)+1 for i in y_]\n",
    "#shapval_rank={}\n",
    "dict_shap_rank= {i:x_sort.index(i)+1 for i in x_}\n",
    "dict_wt_mat={i:y_sort.index(i)+1 for i in y_}\n",
    "\n",
    "bar_labels=[str(i) + '\\n rank\\n sh#'+str(dict_shap_rank[x_[feature_names.index(i)]]) +'\\nwt#'+ str(dict_wt_mat[y_[feature_names.index(i)]]) for i in feature_names ]\n",
    "feature_ranks = [str(i) + ' rank      sh#'+str(dict_shap_rank[x_[feature_names.index(i)]]) +'       wt#'+ str(dict_wt_mat[y_[feature_names.index(i)]]) for i in feature_names ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298a0a9-41c4-4442-8257-50568d7dcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "res_SIP_CIP = stats.spearmanr(x_, y_,)\n",
    "res_DIG_CIP = stats.spearmanr(z_, y_,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bb77a-b7a2-4e0f-90c7-6ce4dbce206b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1fdf0-99a4-413c-b678-9037b94486c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Sample data\n",
    "cor_data = {\n",
    "    'SIP': x_,\n",
    "    'CIP': y_,\n",
    "    'DIAG': z_\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "cor_df = pd.DataFrame(cor_data)\n",
    "\n",
    "# Calculate Spearman correlation matrix\n",
    "corr_matrix, _ = stats.spearmanr(cor_df)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "corr_df = pd.DataFrame(corr_matrix, index=cor_df.columns, columns=cor_df.columns)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Spearman Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bdd4b-bb94-454e-a81f-752a03bf4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3518c6d-a527-43e7-91aa-c5fa1351662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rc(\"xtick\", labelsize=\"small\")\n",
    "matplotlib.rc(\"ytick\", labelsize=\"small\")\n",
    "plot_values ={'shap':x_, 'weight matrix' :y_,'diag':z_}\n",
    "x=np.arange(len(x_))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in plot_values.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Weights & Shapley Values')\n",
    "ax.set_title('Shapley vs Weight Matrix')\n",
    "ax.set_xticks(x + width, bar_labels)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, max(max(y_),max(x_),max(z_))+0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5a7db-2592-4e92-aeeb-9d03472d8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.absolute([[3,0],[1,-2]]),np.absolute([[1,2],[3,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a505d5-93bd-4b7c-b173-a8224be7c182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12444122-7c7e-4103-9b7a-3b8fa4c4c1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac356c-7768-43f3-bce9-4475294ff715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316262d9-c8ed-4964-981a-f06f3ed4ed1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
